{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19b9907-421e-41e8-8ee9-039f0cd30a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "import string\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense,Input,Embedding,Dropout,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization,SimpleRNN,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#import transformers\n",
    "#import tokenizers\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1272d1bd-eb7b-4c2a-b554-c894ba714efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa74244-2355-49b5-871c-b84e902deb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1204ba0-cbe9-4b67-8467-617cd0de27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data is 1600000\n"
     ]
    }
   ],
   "source": [
    "print('length of data is', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e75c251-bbe2-4cfb-8862-1354e6dd553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6d587a-e3b9-4270-addd-db5656293ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a971bf3-82eb-4169-b81a-480ee97e308a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6074df09-daf5-45f2-b73f-89ca7eebe66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'] = data['target'].replace(4,1)\n",
    "data=data[['text','target']]\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cd1362-f734-4dcc-9a1a-3ab0683392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "def clean_text(s):\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n",
    "    s = re.sub(r'@\\S+', '', s)\n",
    "    s = re.sub('&amp', ' ', s)\n",
    "    return s\n",
    "\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()\n",
    "data['text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cb60ad-141b-4181-8fb4-6ab9dc806087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "x=data.text\n",
    "y=data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
    "    test_size=0.05, shuffle = True, random_state = 8)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.1, random_state= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9730398-a6e6-4d62-aa60-96d5fcabafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(X_test) #for vader and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ccdb0f-052e-4919-95f5-cc3d0fff1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "max_features = 40000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c44e41-d07b-4063-ad90-25cba1a1087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequencing for lstm\n",
    "max_words = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words, padding = 'post')\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words, padding = 'post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5360b1-7cb1-4569-a781-2f6196084e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368000, 100) (152000, 100) (80000, 100)\n"
     ]
    }
   ],
   "source": [
    "#Evaluating proper format and shape\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train.shape,X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb6fb19-f201-456c-a086-88ed8af2b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 10)           400000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 32)           992       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 50, 32)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 50, 32)            3104      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 25, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 25, 32)            3104      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 12, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 12, 32)            3104      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 6, 32)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 6, 32)             3104      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 3, 32)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 466709 (1.78 MB)\n",
      "Trainable params: 466709 (1.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(40000, 10, input_length=X_train.shape[1]))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d38aa4-4219-4964-9ca6-9a41ee4b9e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5344/5344 [==============================] - 273s 50ms/step - loss: 0.4313 - accuracy: 0.7993 - val_loss: 0.4075 - val_accuracy: 0.8141\n",
      "Epoch 2/10\n",
      "5344/5344 [==============================] - 255s 48ms/step - loss: 0.3874 - accuracy: 0.8254 - val_loss: 0.4056 - val_accuracy: 0.8152\n",
      "Epoch 3/10\n",
      "5344/5344 [==============================] - 274s 51ms/step - loss: 0.3688 - accuracy: 0.8353 - val_loss: 0.4119 - val_accuracy: 0.8123\n",
      "Epoch 4/10\n",
      "5344/5344 [==============================] - 347s 65ms/step - loss: 0.3549 - accuracy: 0.8426 - val_loss: 0.4164 - val_accuracy: 0.8120\n",
      "Epoch 5/10\n",
      "5344/5344 [==============================] - 345s 65ms/step - loss: 0.3431 - accuracy: 0.8492 - val_loss: 0.4169 - val_accuracy: 0.8104\n",
      "Epoch 6/10\n",
      "5344/5344 [==============================] - 346s 65ms/step - loss: 0.3327 - accuracy: 0.8544 - val_loss: 0.4310 - val_accuracy: 0.8067\n",
      "Epoch 7/10\n",
      "5344/5344 [==============================] - 347s 65ms/step - loss: 0.3235 - accuracy: 0.8586 - val_loss: 0.4532 - val_accuracy: 0.8074\n",
      "Epoch 8/10\n",
      "5344/5344 [==============================] - 336s 63ms/step - loss: 0.3151 - accuracy: 0.8626 - val_loss: 0.4597 - val_accuracy: 0.8045\n",
      "Epoch 9/10\n",
      "5344/5344 [==============================] - 341s 64ms/step - loss: 0.3075 - accuracy: 0.8661 - val_loss: 0.4715 - val_accuracy: 0.8030\n",
      "Epoch 10/10\n",
      "5344/5344 [==============================] - 339s 63ms/step - loss: 0.3006 - accuracy: 0.8691 - val_loss: 0.4920 - val_accuracy: 0.8016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c9b446a10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b2bb87-9d05-493a-9c01-234abc4f0a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 17s 7ms/step - loss: 0.4904 - accuracy: 0.8019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4903734028339386, 0.8018875122070312]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(X_test,y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3b818c-ba63-4ae5-8190-3c3d32fd5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbed48a8-29dd-452b-a87e-64285e8ed496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\samar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc5d412-5a5d-465c-b3df-4eeecfeb1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.sentences = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                sentence = line.strip().split(' ')\n",
    "                self.sentences.append(sentence)\n",
    "\n",
    "        return iter(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d846d0d1-8085-4609-a515-9fb82d0c8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentences = MySentences('sentiment140.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07d59a58-0026-489b-880d-816ed0ed5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "522d1e2b-4b1a-4600-ac97-d13141b60d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0834a996-456d-4354-b34d-4cca91bf4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = \"glove.6B.100d.txt\"\n",
    "word2vec_output_file = \"glove.6B.100d.word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68cf86e-f52c-48aa-81e8-6ce14cf0db28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2word2vec(glove_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bdf141d-c889-44aa-8a4e-f015ce6b02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef37e53-3246-43b0-9d0a-e7bd2edaf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector(embedding_size=50,window_size=5,training_epochs=5,initial_lr=0.025,min_lr=0.0001, data_path='sentiment140.csv'):\n",
    "    \"\"\"\n",
    "    generate word vectors\n",
    "    \"\"\"\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(MySentences(data_path),\n",
    "                               size=embedding_size,window=window_size,iter=training_epochs,\n",
    "                               alpha=initial_lr,min_alpha=min_lr,\n",
    "                               sg=1, min_count=2, workers=4, hs=0, negative=10)\n",
    "    model_path=os.path.join(\"word2vec\", \"model-\" + str(embedding_size))\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88588701-09e1-4e39-ad83-41af46c9ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = 100  \n",
    "max_features = 40000  # Number of unique words\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_features:\n",
    "        try:\n",
    "            embedding_vector = glove_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not found in GLOVE, use a random vector or zeros\n",
    "            embedding_matrix[i] = np.random.normal(0, 1, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12bb6a53-a7cb-4ce3-860c-5b7e6ba70e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional, Attention, Concatenate, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e6a05d74-e101-44dc-9f01-d1678080ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANModel(Model):\n",
    "    def __init__(self, max_words, max_sentences, embedding_matrix, word_hidden_units, sentence_hidden_units, num_classes):\n",
    "        super(HANModel, self).__init__()\n",
    "\n",
    "        self.max_words = max_words\n",
    "        self.max_sentences = max_sentences\n",
    "        self.word_hidden_units = word_hidden_units\n",
    "        self.sentence_hidden_units = sentence_hidden_units\n",
    "\n",
    "        # Word-level attention\n",
    "        self.word_attention = Attention(use_scale=True)\n",
    "        self.word_context = Dense(word_hidden_units, activation='tanh')\n",
    "\n",
    "        # Sentence-level attention\n",
    "        self.sentence_attention = Attention(use_scale=True)\n",
    "        self.sentence_context = Dense(sentence_hidden_units, activation='tanh') \n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_words,\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        # Word-level LSTM\n",
    "        self.word_lstm = Bidirectional(LSTM(word_hidden_units, return_sequences=True))\n",
    "\n",
    "        # Sentence-level LSTM\n",
    "        self.sentence_lstm = Bidirectional(LSTM(sentence_hidden_units, return_sequences=True))\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = Dense(128, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Word-level attention\n",
    "        word_representations = self.embedding(inputs)\n",
    "        word_representations = self.word_lstm(word_representations)\n",
    "\n",
    "        word_attention_weights = self.word_attention([word_representations, word_representations])\n",
    "        word_representations = Concatenate(axis=-1)([word_representations, word_attention_weights])\n",
    "        word_representations = self.word_context(word_representations)\n",
    "\n",
    "        # Sentence-level attention\n",
    "        sentence_representations = self.sentence_lstm(word_representations)\n",
    "\n",
    "        sentence_attention_weights = self.sentence_attention([sentence_representations, sentence_representations])\n",
    "        sentence_representations = Concatenate(axis=-1)([sentence_representations, sentence_attention_weights])\n",
    "        sentence_representations = self.sentence_context(sentence_representations)\n",
    "\n",
    "        # Classification layers\n",
    "        avg_sentence_representations = tf.reduce_mean(sentence_representations, axis=1)\n",
    "        x = self.fc1(avg_sentence_representations)\n",
    "        x = self.fc2(x)\n",
    "        outputs = self.output_layer(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a51061ac-de14-42cf-9aad-a9e0d8c9be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100 \n",
    "max_sentences = 10  \n",
    "embedding_dim = 100  \n",
    "word_hidden_units = 64  \n",
    "sentence_hidden_units = 64 \n",
    "num_classes = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a3d4a0b-8624-468e-9f67-d020e4bbc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "han_model = HANModel(max_words, max_sentences, embedding_matrix, word_hidden_units, sentence_hidden_units, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3290deb4-decc-4517-905d-4c682efb5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "han_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d954acb-cf27-4ef6-ab20-9e9e5e8ef6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42750/42750 [==============================] - 7125s 166ms/step - loss: 0.4460 - accuracy: 0.7907 - val_loss: 0.4195 - val_accuracy: 0.8061\n",
      "Epoch 2/10\n",
      "42750/42750 [==============================] - 22483s 526ms/step - loss: 0.4065 - accuracy: 0.8143 - val_loss: 0.4108 - val_accuracy: 0.8102\n",
      "Epoch 3/10\n",
      "42750/42750 [==============================] - 5631s 132ms/step - loss: 0.3939 - accuracy: 0.8210 - val_loss: 0.4042 - val_accuracy: 0.8149\n",
      "Epoch 4/10\n",
      "42750/42750 [==============================] - 6843s 160ms/step - loss: 0.3851 - accuracy: 0.8258 - val_loss: 0.4065 - val_accuracy: 0.8144\n",
      "Epoch 5/10\n",
      "42750/42750 [==============================] - 10821s 253ms/step - loss: 0.3786 - accuracy: 0.8292 - val_loss: 0.4027 - val_accuracy: 0.8156\n",
      "Epoch 6/10\n",
      "42750/42750 [==============================] - 9591s 224ms/step - loss: 0.3736 - accuracy: 0.8319 - val_loss: 0.4053 - val_accuracy: 0.8156\n",
      "Epoch 7/10\n",
      "42750/42750 [==============================] - 63869s 1s/step - loss: 0.3692 - accuracy: 0.8343 - val_loss: 0.4021 - val_accuracy: 0.8168\n",
      "Epoch 8/10\n",
      "42750/42750 [==============================] - 20715s 485ms/step - loss: 0.3659 - accuracy: 0.8360 - val_loss: 0.4062 - val_accuracy: 0.8148\n",
      "Epoch 9/10\n",
      "42750/42750 [==============================] - 27283s 638ms/step - loss: 0.3628 - accuracy: 0.8378 - val_loss: 0.4075 - val_accuracy: 0.8149\n",
      "Epoch 10/10\n",
      "42750/42750 [==============================] - 7029s 164ms/step - loss: 0.3606 - accuracy: 0.8390 - val_loss: 0.4055 - val_accuracy: 0.8144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22b0effa890>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "han_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4bc10263-3425-46fc-a007-6d2b106b9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 151s 60ms/step - loss: 0.4056 - accuracy: 0.8168\n",
      "Test Accuracy: 0.8167999982833862\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = han_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dcf04-7585-4cd0-b9b1-e75f3f4fe69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1ec6c-a5e3-46d6-8d1e-9ec2594e6133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
